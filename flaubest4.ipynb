{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7058876,"sourceType":"datasetVersion","datasetId":4063532}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importation des bibliothèques nécessaires\n!pip install transformers\n!pip install tensorflow\n!pip install sacremoses\n!pip install sentencepiece\n\n\nimport pandas as pd\nimport re\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import FlaubertTokenizer, TFFlaubertModel, TFFlaubertForSequenceClassification\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-19T23:09:51.502260Z","iopub.execute_input":"2023-12-19T23:09:51.502652Z","iopub.status.idle":"2023-12-19T23:10:38.045814Z","shell.execute_reply.started":"2023-12-19T23:09:51.502618Z","shell.execute_reply":"2023-12-19T23:10:38.044518Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.13.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.9.0)\nRequirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.1)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.24.3)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (68.1.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.0)\nRequirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.5.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.34.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.10/site-packages (0.1.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacremoses) (2023.8.8)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.3.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sacremoses) (4.66.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Charger les données\ntrain_data = pd.read_csv('/kaggle/input/ouchy-data/training_data.csv')\n\n# Dupliquer les données une première fois\ntrain_data_duplicated_once = pd.concat([train_data, train_data])\n\n# Dupliquer les données une deuxième fois pour obtenir une multiplication par 4\ntrain_data_duplicated_twice = pd.concat([train_data_duplicated_once, train_data_duplicated_once])\n\ntrain_data = train_data_duplicated_twice\n\nX = train_data['sentence']\ny = train_data['difficulty']\n\n# Encodage des labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Diviser les données en ensembles d'entraînement et de test\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Initialiser le tokenizer FlauBERT\ntokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n\n# Préparer les données pour FlauBERT\ndef encode_for_flaubert(sentences, max_length=128):\n    input_ids = []\n    attention_masks = []\n\n    for sentence in sentences:\n        encoded_dict = tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=max_length,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='tf',\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = tf.concat(input_ids, 0)\n    attention_masks = tf.concat(attention_masks, 0)\n\n    return input_ids, attention_masks\n\ntrain_input_ids, train_attention_masks = encode_for_flaubert(X_train, max_length=128)\ntest_input_ids, test_attention_masks = encode_for_flaubert(X_test, max_length=128)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:10:38.048112Z","iopub.execute_input":"2023-12-19T23:10:38.048455Z","iopub.status.idle":"2023-12-19T23:10:56.660532Z","shell.execute_reply.started":"2023-12-19T23:10:38.048425Z","shell.execute_reply":"2023-12-19T23:10:56.659670Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Charger le modèle FlauBERT pré-entraîné pour la classification de séquence\nmodel = TFFlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=len(label_encoder.classes_), from_pt=True)\n\n# Compiler le modèle\noptimizer = Adam(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\n# Entraîner le modèle\nmodel.fit(\n    [train_input_ids, train_attention_masks],\n    y_train,\n    epochs=3,\n    batch_size=16,\n    validation_data=([test_input_ids, test_attention_masks], y_test)\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:10:56.661750Z","iopub.execute_input":"2023-12-19T23:10:56.662022Z","iopub.status.idle":"2023-12-19T23:32:01.676954Z","shell.execute_reply.started":"2023-12-19T23:10:56.661999Z","shell.execute_reply":"2023-12-19T23:32:01.675991Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFFlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n- This IS expected if you are initializing TFFlaubertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFFlaubertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFFlaubertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n960/960 [==============================] - 447s 429ms/step - loss: 1.0277 - accuracy: 0.5695 - val_loss: 0.6325 - val_accuracy: 0.7552\nEpoch 2/3\n960/960 [==============================] - 408s 425ms/step - loss: 0.5045 - accuracy: 0.8081 - val_loss: 0.3311 - val_accuracy: 0.8836\nEpoch 3/3\n960/960 [==============================] - 408s 425ms/step - loss: 0.3469 - accuracy: 0.8745 - val_loss: 0.2466 - val_accuracy: 0.9154\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7b7c0c92ff70>"},"metadata":{}}]},{"cell_type":"code","source":"# Charger les données de test non étiquetées\ntest_data = pd.read_csv('/kaggle/input/ouchy-data/unlabelled_test_data.csv')\ntest_input_ids, test_attention_masks = encode_for_flaubert(test_data['sentence'])\n\n# Prédiction sur les données de test\ntest_predictions = model.predict([test_input_ids, test_attention_masks])\ntest_predicted_classes = tf.argmax(test_predictions.logits, axis=1).numpy()\n\n# Création du fichier de soumission\nsubmission = pd.DataFrame({'id': test_data['id'], 'difficulty': label_encoder.inverse_transform(test_predicted_classes)})\nsubmission.to_csv('submission3.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:32:01.679284Z","iopub.execute_input":"2023-12-19T23:32:01.679716Z","iopub.status.idle":"2023-12-19T23:32:15.938500Z","shell.execute_reply.started":"2023-12-19T23:32:01.679678Z","shell.execute_reply":"2023-12-19T23:32:15.936062Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"38/38 [==============================] - 13s 252ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import HTML\n\n# Supposons que vous ayez un ensemble de test X_test et y_test\ntest_input_ids, test_attention_masks = encode_for_flaubert(X_test, max_length=128)\ny_pred = model.predict([test_input_ids, test_attention_masks])\ny_pred_classes = np.argmax(y_pred.logits, axis=1)\n\n# Calculer les métriques\nprecision = precision_score(y_test, y_pred_classes, average='weighted')\nrecall = recall_score(y_test, y_pred_classes, average='weighted')\nf1 = f1_score(y_test, y_pred_classes, average='weighted')\naccuracy = accuracy_score(y_test, y_pred_classes)\n\n# Générer la matrice de confusion\ncm = confusion_matrix(y_test, y_pred_classes)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:32:15.940315Z","iopub.execute_input":"2023-12-19T23:32:15.940716Z","iopub.status.idle":"2023-12-19T23:32:50.838658Z","shell.execute_reply.started":"2023-12-19T23:32:15.940682Z","shell.execute_reply":"2023-12-19T23:32:50.837911Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"120/120 [==============================] - 31s 259ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tableau des métriques\nmetrics_html = f\"\"\"\n<table>\n<tr><th>Metric</th><th>Value</th></tr>\n<tr><td>Precision</td><td>{precision:.2f}</td></tr>\n<tr><td>Recall</td><td>{recall:.2f}</td></tr>\n<tr><td>F1-Score</td><td>{f1:.2f}</td></tr>\n<tr><td>Accuracy</td><td>{accuracy:.2f}</td></tr>\n</table>\n\"\"\"\n\n# Tableau de la matrice de confusion\ncm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)\ncm_html = cm_df.to_html()\n\n# Afficher les tableaux\nHTML(metrics_html + cm_html)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:33:19.736748Z","iopub.execute_input":"2023-12-19T23:33:19.737156Z","iopub.status.idle":"2023-12-19T23:33:19.750528Z","shell.execute_reply.started":"2023-12-19T23:33:19.737122Z","shell.execute_reply":"2023-12-19T23:33:19.749357Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<table>\n<tr><th>Metric</th><th>Value</th></tr>\n<tr><td>Precision</td><td>0.92</td></tr>\n<tr><td>Recall</td><td>0.92</td></tr>\n<tr><td>F1-Score</td><td>0.92</td></tr>\n<tr><td>Accuracy</td><td>0.92</td></tr>\n</table>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A1</th>\n      <th>A2</th>\n      <th>B1</th>\n      <th>B2</th>\n      <th>C1</th>\n      <th>C2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A1</th>\n      <td>631</td>\n      <td>24</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A2</th>\n      <td>65</td>\n      <td>563</td>\n      <td>19</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>B1</th>\n      <td>8</td>\n      <td>84</td>\n      <td>558</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>B2</th>\n      <td>1</td>\n      <td>8</td>\n      <td>44</td>\n      <td>559</td>\n      <td>6</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>C1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>13</td>\n      <td>597</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>C2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>8</td>\n      <td>17</td>\n      <td>607</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"# Préparer les données pour les prédictions erronées\nincorrect_predictions_df = pd.DataFrame({'Sentence': X_test, 'Actual': label_encoder.inverse_transform(y_test), 'Predicted': label_encoder.inverse_transform(y_pred_classes)})\nincorrect_predictions_df = incorrect_predictions_df[incorrect_predictions_df['Actual'] != incorrect_predictions_df['Predicted']].head(10)\n\n# Convertir en HTML\nincorrect_html = incorrect_predictions_df.to_html()\nHTML(incorrect_html)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T00:09:58.673112Z","iopub.execute_input":"2023-12-20T00:09:58.674015Z","iopub.status.idle":"2023-12-20T00:09:58.689939Z","shell.execute_reply.started":"2023-12-20T00:09:58.673970Z","shell.execute_reply":"2023-12-20T00:09:58.688959Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Actual</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4180</th>\n      <td>Il fait un froid incroyable à Moscou mais la ville est superbe</td>\n      <td>B1</td>\n      <td>A2</td>\n    </tr>\n    <tr>\n      <th>1960</th>\n      <td>Malgré le danger, il a gardé la tête froide durant cette expérience.</td>\n      <td>B2</td>\n      <td>B1</td>\n    </tr>\n    <tr>\n      <th>2609</th>\n      <td>Le nombre insuffisant de professeurs formés est également un frein pour beaucoup d'adversaires de cette mesure, qui la jugent inapplicable.</td>\n      <td>C2</td>\n      <td>B2</td>\n    </tr>\n    <tr>\n      <th>1347</th>\n      <td>Cette période marque l'arrivée de Noël et des fêtes de fin d'année.</td>\n      <td>A1</td>\n      <td>A2</td>\n    </tr>\n    <tr>\n      <th>2920</th>\n      <td>Oui, prenons-en pour le trajet!</td>\n      <td>A2</td>\n      <td>A1</td>\n    </tr>\n    <tr>\n      <th>1277</th>\n      <td>Je m'appelle Laurent et j'habite à Paris avec mes parents, ma soeur ainée et mon frère.</td>\n      <td>A2</td>\n      <td>A1</td>\n    </tr>\n    <tr>\n      <th>344</th>\n      <td>Comment j'ai survécu à une perverse narcissique</td>\n      <td>B2</td>\n      <td>B1</td>\n    </tr>\n    <tr>\n      <th>1411</th>\n      <td>L'asile est le creuset terrible où se forge l'identité du vagabond.</td>\n      <td>C1</td>\n      <td>C2</td>\n    </tr>\n    <tr>\n      <th>4591</th>\n      <td>Les genoux au menton, les bras croisés sur la poitrine, il se fit boule pour mieux interroger le mot qui ne quittait jamais longtemps sa pensée.</td>\n      <td>C1</td>\n      <td>C2</td>\n    </tr>\n    <tr>\n      <th>2684</th>\n      <td>C'est une chance pour lui, il va gagner un peu plus d'argent.</td>\n      <td>B1</td>\n      <td>A1</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install streamlit\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T00:15:21.618407Z","iopub.execute_input":"2023-12-20T00:15:21.619322Z","iopub.status.idle":"2023-12-20T00:15:35.730517Z","shell.execute_reply.started":"2023-12-20T00:15:21.619288Z","shell.execute_reply":"2023-12-20T00:15:35.729531Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Collecting streamlit\n  Obtaining dependency information for streamlit from https://files.pythonhosted.org/packages/d3/96/9251b421d0a1c7d625a82a04bea56b8a9830c785940ec16db454b85c6db7/streamlit-1.29.0-py2.py3-none-any.whl.metadata\n  Downloading streamlit-1.29.0-py2.py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (5.2.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.7.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.2.4)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.1.7)\nRequirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/lib/python3.10/site-packages (from streamlit) (6.8.0)\nRequirement already satisfied: numpy<2,>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.24.3)\nRequirement already satisfied: packaging<24,>=16.8 in /opt/conda/lib/python3.10/site-packages (from streamlit) (21.3)\nRequirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.0.3)\nRequirement already satisfied: pillow<11,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (10.1.0)\nRequirement already satisfied: protobuf<5,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=6.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (11.0.0)\nRequirement already satisfied: python-dateutil<3,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.8.2)\nRequirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.31.0)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (13.5.2)\nRequirement already satisfied: tenacity<9,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.2.3)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.5.0)\nRequirement already satisfied: tzlocal<6,>=1.1 in /opt/conda/lib/python3.10/site-packages (from streamlit) (5.2)\nCollecting validators<1,>=0.2 (from streamlit)\n  Obtaining dependency information for validators<1,>=0.2 from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.1.32)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (6.3.3)\nCollecting watchdog>=2.1.5 (from streamlit)\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.19.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.10)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit) (3.16.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<24,>=16.8->streamlit) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.9.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\nDownloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading validators-0.22.0-py3-none-any.whl (26 kB)\nInstalling collected packages: watchdog, validators, pydeck, streamlit\nSuccessfully installed pydeck-0.8.1b0 streamlit-1.29.0 validators-0.22.0 watchdog-3.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import streamlit as st\nimport types\nfrom transformers import FlaubertTokenizer, TFFlaubertForSequenceClassification\nimport tensorflow as tf\nimport numpy as np\n\n# Custom hash function to bypass hashing of the load_model function\ndef bypass_hashing(func):\n    return 0\n\n# Function to load the FlauBERT model\n@st.cache(allow_output_mutation=True, hash_funcs={types.FunctionType: bypass_hashing})\ndef load_model():\n    model = TFFlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=6, from_pt=True)\n    return model\n\n# Function to encode text for FlauBERT\ndef encode_text(text, tokenizer, max_length=128):\n    encoded_dict = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='tf',\n    )\n    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n\n# Load FlauBERT tokenizer\ntokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n\n# Load the model\nmodel = load_model()\n\n# Streamlit interface\nst.title('French Text Difficulty Predictor')\nuser_input = st.text_area(\"Enter a sentence in French\", \"\")\n\nif st.button('Predict Difficulty'):\n    input_ids, attention_masks = encode_text(user_input, tokenizer)\n    predictions = model.predict([input_ids, attention_masks])\n    difficulty_level = np.argmax(predictions.logits, axis=1)[0]\n\n    # Mapping the prediction to difficulty level\n    levels = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n    predicted_level = levels[difficulty_level]\n\n    st.write(f\"The predicted difficulty level is: {predicted_level}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T00:51:58.597412Z","iopub.execute_input":"2023-12-20T00:51:58.597836Z","iopub.status.idle":"2023-12-20T00:52:00.555654Z","shell.execute_reply.started":"2023-12-20T00:51:58.597809Z","shell.execute_reply":"2023-12-20T00:52:00.554936Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFFlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n- This IS expected if you are initializing TFFlaubertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFFlaubertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFFlaubertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2023-12-20 00:52:00.551 `st.cache` is deprecated. Please use one of Streamlit's new caching commands,\n`st.cache_data` or `st.cache_resource`.\n\nMore information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model\nmodel_save_path = \"/Users/mac/Desktop/KAGGLE Competition\"\nmodel.save_pretrained(model_save_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T01:48:15.371341Z","iopub.execute_input":"2023-12-20T01:48:15.372430Z","iopub.status.idle":"2023-12-20T01:48:17.514002Z","shell.execute_reply.started":"2023-12-20T01:48:15.372395Z","shell.execute_reply":"2023-12-20T01:48:17.512930Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model_save_path = \"/Users/mac/Desktop/my_flauBERT_model\"\nmodel.save(model_save_path, save_format=\"tf\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T01:50:50.957878Z","iopub.execute_input":"2023-12-20T01:50:50.958225Z","iopub.status.idle":"2023-12-20T01:51:16.508508Z","shell.execute_reply.started":"2023-12-20T01:50:50.958199Z","shell.execute_reply":"2023-12-20T01:51:16.507530Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model_save_path = \"./my_flauBERT_model\"\nmodel.save(model_save_path, save_format=\"tf\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T01:52:59.311110Z","iopub.execute_input":"2023-12-20T01:52:59.311972Z","iopub.status.idle":"2023-12-20T01:53:24.342611Z","shell.execute_reply.started":"2023-12-20T01:52:59.311925Z","shell.execute_reply":"2023-12-20T01:53:24.341801Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"!zip -r my_flauBERT_model.zip my_flauBERT_model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T01:57:29.097106Z","iopub.execute_input":"2023-12-20T01:57:29.097498Z","iopub.status.idle":"2023-12-20T01:57:58.783144Z","shell.execute_reply.started":"2023-12-20T01:57:29.097460Z","shell.execute_reply":"2023-12-20T01:57:58.781947Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"  adding: my_flauBERT_model/ (stored 0%)\n  adding: my_flauBERT_model/keras_metadata.pb (deflated 95%)\n  adding: my_flauBERT_model/saved_model.pb (deflated 92%)\n  adding: my_flauBERT_model/assets/ (stored 0%)\n  adding: my_flauBERT_model/variables/ (stored 0%)\n  adding: my_flauBERT_model/variables/variables.index (deflated 77%)\n  adding: my_flauBERT_model/variables/variables.data-00000-of-00001 (deflated 7%)\n  adding: my_flauBERT_model/fingerprint.pb (stored 0%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'my_flauBERT_model.zip')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T02:04:41.950527Z","iopub.execute_input":"2023-12-20T02:04:41.951238Z","iopub.status.idle":"2023-12-20T02:04:41.958166Z","shell.execute_reply.started":"2023-12-20T02:04:41.951194Z","shell.execute_reply":"2023-12-20T02:04:41.957265Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/my_flauBERT_model.zip","text/html":"<a href='my_flauBERT_model.zip' target='_blank'>my_flauBERT_model.zip</a><br>"},"metadata":{}}]}]}